# HuggingFace-Transformers_and_pipeline

This repo contains four Jupyter notebooks that demonstrate how to use HuggingFace Transformers and pipeline for various natural language processing tasks.

## 1. Hugging_face_Transformer_pipeline.ipynb:
 - This notebook shows how to use the pipeline function to easily apply pre-trained models for tasks such as text classification, named entity recognition, question answering, text generation and summarization.
## 2. Sentiment_Analysis.ipynb: 
- This notebook shows how to fine-tune a pre-trained model for sentiment analysis on the IMDB movie reviews dataset. It also shows how to use the Trainer class and the datasets library to simplify the training and evaluation process.
## 3. Simple_QA_USING_BERT.ipynb: 
- This notebook shows how to build a simple question answering system using BERT and the SQuAD dataset. It also shows how to use the transformers library to load and preprocess the data, create the model and tokenizer, and evaluate the results.
## 4. Transformers_From_Scratch.ipynb: 
- This notebook shows how to implement a transformer model from scratch using PyTorch. It explains the basic components of a transformer, such as the attention mechanism, the encoder and decoder, and the positional encoding. It also shows how to train the model on a toy translation task.

# Requirements
To run the notebooks, you will need the following packages:

- `transformers`
- `datasets`
- `torch` , etc.
- You can install them using `pip` or `conda`

# Usage
   You can clone this repo and open the notebooks in your preferred environment, such as Google Colab, Jupyter Lab, or VS Code. You can also view them on GitHub or on nbviewer.

